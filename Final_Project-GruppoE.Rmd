# Progetto Finale - Gruppo E
#### Giacomo Serafini, Enrico Guerriero, Leonardo Marsich, Gilberto Bran

## Data Set
Il dataset *WiscNursingHome* contiene le informazioni di più di 350 case di riposo, che verranno utilizzate per capire come si relaziona il numero di pazienti rispetto alle caratteristiche della struttura.   
Nel dataset sono presenti 12 variabili:

* *hospID*: l'ID rappresentativo della singola struttura (quantitativa)
* *CRYEAR*: l'anno del rapporto dei costi (categoriale)
* *TPY*: il totale pazienti annui (quantitativa)
* *NUMBED*: il numero di letti (quantitativa)
* *SQRFOOT*: il numero di piedi quadrati della struttura (quantitativa)
* *MSA*: codice dell'area statistica metropolitana (area divisa in 13 zone, più lo 0 se rurale) (categoriale)
* *URBAN*: 1 se urbana, 0 se rurale (categoriale)
* *PRO*: 0 se non-profit, 1 altrimenti (categoriale)
* *TAXEXEMPT*: 1 se esente dalle tasse, 0 altrimenti (categoriale)
* *SELFFUNDINS*: 1 se autofinanziato per l'assicurazione, 0 altrimenti (categoriale)
* *MCERT*: 1 se certificato Medicare, 0 altrimenti (categoriale)
* *ORGSTR*: 1 se con finalità di lucro, 2 se esente dalle tasse, 3 se unità governativa (categoriale)

L'obiettivo della nostra analisi è di capire come le caratteristiche di una casa di riposo sono tra loro collegate (se collegate in alcun modo), per comprendere quali sono le principali peculiarità di una struttura che portano ad avere un numero maggiore di pazienti.

```{r include=FALSE}
# Richiamo delle librerie
library(corrplot)
library(ggplot2)
library(cowplot)
library(dplyr)
library(factoextra)
library(cluster)
library(grid)

# Creazione del dataset da WiscNursingHome
Data <- read.csv("WiscNursingHome.csv", header = TRUE)

# Fattorizzazione delle variabili categoriali
Data$CRYEAR <- factor(Data$CRYEAR)
Data$MSA <- factor(Data$MSA)
Data$URBAN <- factor(Data$URBAN)
Data$PRO <- factor(Data$PRO)
Data$TAXEXEMPT <- factor(Data$TAXEXEMPT)
Data$SELFFUNDINS <- factor(Data$SELFFUNDINS)
Data$MCERT <- factor(Data$MCERT)
Data$ORGSTR <- factor(Data$ORGSTR)

# Ci sono alcuni dati mancanti?
na.id.Data <- apply(is.na(Data), 2, which) 
# Abbiamo 10 dati mancanti in SQRFOOT
na.SQRFOOT <- na.id.Data$SQRFOOT
# Data frame senza NA
DataNa <- Data[-na.SQRFOOT,]
```

Visualizziamo il dataset:

```{r, include = T, echo = F}
summary(Data)
```

Nel dataset sono presenti solo 10 dati mancanti nella colonna *SQRFOOT*.


## Variabile Risposta

La variabile risposta individuata è la variabile *TPY*, ovvero il numero totale di pazienti ospitati nella casa di riposo in un anno:

```{r, include = T, echo = F}
summary(Data$TPY)
r1 <- ggplot(data = Data, aes(y = TPY)) +
  geom_boxplot(fill = "yellow") +
  theme_bw() +
  labs(y = "TPY") +
  theme(plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
        axis.title = element_text(size = 14),
        axis.text = element_text(size = 12),
        legend.title = element_blank(),
        legend.text = element_text(size = 12),
        axis.ticks.x = element_blank(),
        axis.text.x = element_blank()) +
  xlim(-0.8,0.8) +
  geom_segment(data  = Data, aes(x = -0.375, xend = 0.375, y = mean(TPY), yend = mean(TPY)),
               linewidth = 1, linetype = "dashed") +
  xlab("")+
  geom_text(data = Data, aes(x = 0.4, y = mean(TPY), label = sprintf("Media")),
            hjust = 0, vjust = 0) +
  geom_text(data = Data, aes(x = 0.4, y = median(TPY) - 10,
                             label = sprintf("Mediana")),
            hjust = 0, vjust = 0)
r2 <- ggplot(data = Data, aes(x = TPY)) +
  geom_histogram(aes(y = after_stat(density)), col = "black", fill = "yellow", bins = 20) +
  geom_density(linewidth = 0.8, fill = "pink", alpha = 0.3) +
  theme_bw() +
  labs(y = "TPY", x = "") +
  theme(plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
        axis.title = element_text(size = 14),
        axis.text = element_text(size = 12),
        legend.title = element_blank(),
        legend.text = element_text(size = 12),
        axis.ticks.x = element_blank(),
        axis.text.x = element_blank())
title <- ggdraw() + 
  draw_label(
    "Distribuzione di TPY",
    fontface = 'bold',
    x = 0,
    hjust = 0
  ) +
  theme(
    plot.margin = margin(0, 0, 0, 7)
  )
rowsplot <- plot_grid(r1, r2,
          nrow = 1)
plot_grid(title,
          rowsplot,
          ncol = 1,
          rel_heights = c(0.1, 1))
```

Si osserva un'assimmetria destra nella distribuzione di *TPY*: la coda destra è molto lunga, come si vede da entrambi i grafici; inoltre, la media è superiore alla mediana.
Tra le trasformazioni effettuate alla variabile che possono renderla simmetrica, la migliore che è stata individuata è la trasformazione logaritmica:

```{r, include = T, echo = F}
summary(log(Data$TPY))
r1 <- ggplot(data = Data, aes(y = log(TPY))) +
  geom_boxplot(fill = "yellow") +
  theme_bw() +
  labs(y = "log(TPY)") +
  theme(plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
        axis.title = element_text(size = 14),
        axis.text = element_text(size = 12),
        legend.title = element_blank(),
        legend.text = element_text(size = 12),
        axis.ticks.x = element_blank(),
        axis.text.x = element_blank()) +
  xlim(-0.8,0.8) +
  geom_segment(data  = Data, aes(x = -0.375, xend = 0.375,
                                 y = mean(log(TPY)), yend = mean(log(TPY))),
               linewidth = 1, linetype = "dashed") +
  xlab("")+
  geom_text(data = Data, aes(x = 0.4, y = mean(log(TPY)-0.1), label = sprintf("Media")),
            hjust = 0, vjust = 0) +
  geom_text(data = Data, aes(x = 0.4, y = median(log(TPY))+0.05,
                             label = sprintf("Mediana")),
            hjust = 0, vjust = 0)
r2 <- ggplot(data = Data, aes(x = log(TPY))) +
  geom_histogram(aes(y = after_stat(density)), col = "black", fill = "yellow", bins = 20) +
  geom_density(linewidth = 0.8, fill = "pink", alpha = 0.3) +
  theme_bw() +
  labs(y = "log(TPY)", x = "") +
  theme(plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
        axis.title = element_text(size = 14),
        axis.text = element_text(size = 12),
        legend.title = element_blank(),
        legend.text = element_text(size = 12),
        axis.ticks.x = element_blank(),
        axis.text.x = element_blank())
title <- ggdraw() + 
  draw_label(
    "Distribuzione del logaritmo di TPY",
    fontface = 'bold',
    x = 0,
    hjust = 0
  ) +
  theme(
    plot.margin = margin(0, 0, 0, 7)
  )
rowsplot <- plot_grid(r1, r2,
          nrow = 1)
plot_grid(title,
          rowsplot,
          ncol = 1,
          rel_heights = c(0.1, 1))
```



## Variabili Quantitative

Iniziamo a vedere le variabili esplicative quantitative, quindi *NUMBED* e *SQRFOOT*.

* Summary NUMBED

```{r echo=FALSE}
summary((Data$NUMBED))
```

* Sumamry SQRFOOT

```{r echo=FALSE}
summary((Data$SQRFOOT))
```


```{r, include = T, echo = F}
c1 <- ggplot(data = Data, aes(y = NUMBED)) +
  geom_boxplot(fill = "yellow") +
  theme_bw() +
  labs(y = "NUMBED") +
  theme(plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
        axis.title = element_text(size = 14),
        axis.text = element_text(size = 12),
        legend.title = element_blank(),
        legend.text = element_text(size = 12),
        axis.ticks.x = element_blank(),
        axis.text.x = element_blank()) +
  xlim(-0.8,0.8) +
  geom_segment(data  = Data, aes(x = -0.375, xend = 0.375, y = mean(NUMBED), yend = mean(NUMBED)),
               linewidth = 1, linetype = "dashed") +
  xlab("")+
  geom_text(data = Data, aes(x = 0.4, y = mean(NUMBED), label = sprintf("Media")),
            hjust = 0, vjust = 0) +
  geom_text(data = Data, aes(x = 0.4, y = median(NUMBED) - 25,
                             label = sprintf("Mediana")),
            hjust = 0, vjust = 0)
c2 <- ggplot(data = Data, aes(x = NUMBED)) +
  geom_histogram(aes(y = after_stat(density)), col = "black", fill = "yellow", bins = 20) +
  geom_density(linewidth = 0.8, fill = "pink", alpha = 0.3) +
  theme_bw() +
  labs(y = "NUMBED", x = "") +
  theme(plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
        axis.title = element_text(size = 14),
        axis.text = element_text(size = 12),
        legend.title = element_blank(),
        legend.text = element_text(size = 12),
        axis.ticks.x = element_blank(),
        axis.text.x = element_blank())
c3 <- ggplot(data = DataNa, aes(y = SQRFOOT)) +
  geom_boxplot(fill = "yellow") +
  theme_bw() +
  labs(y = "SQRFOOT") +
  theme(plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
        axis.title = element_text(size = 14),
        axis.text = element_text(size = 12),
        legend.title = element_blank(),
        legend.text = element_text(size = 12),
        axis.ticks.x = element_blank(),
        axis.text.x = element_blank()) +
  xlim(-0.8,0.8) +
  geom_segment(data  = DataNa, aes(x = -0.375, xend = 0.375, y = mean(SQRFOOT), yend = mean(SQRFOOT)),
               linewidth = 1, linetype = "dashed") +
  xlab("")+
  geom_text(data = DataNa, aes(x = 0.4, y = mean(SQRFOOT), label = sprintf("Media")),
            hjust = 0, vjust = 0) +
  geom_text(data = DataNa, aes(x = 0.4, y = median(SQRFOOT) - 10,
                             label = sprintf("Mediana")),
            hjust = 0, vjust = 0)
c4 <- ggplot(data = DataNa, aes(x = SQRFOOT)) +
  geom_histogram(aes(y = after_stat(density)), col = "black", fill = "yellow", bins = 20) +
  geom_density(linewidth = 0.8, fill = "pink", alpha = 0.3) +
  theme_bw() +
  labs(y = "SQRFOOT", x = "") +
  theme(plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
        axis.title = element_text(size = 14),
        axis.text = element_text(size = 12),
        legend.title = element_blank(),
        legend.text = element_text(size = 12),
        axis.ticks.x = element_blank(),
        axis.text.x = element_blank())
title <- ggdraw() + 
  draw_label(
    "Distribuzione di NUMBED e SQRFOOT",
    fontface = 'bold',
    x = 0,
    hjust = 0
  ) +
  theme(
    plot.margin = margin(0, 0, 0, 7)
  )
rowsplot <- plot_grid(c1, c2,
                      c3, c4,
          nrow = 2)
plot_grid(title,
          rowsplot,
          ncol = 1,
          rel_heights = c(0.1, 2))
```

Si osserva una forte asimmetria analoga a *TPY* anche nelle distribuzioni delle variabili esplicative, in particolare in *SQRFOOT*.

Analizziamo ora le correlazioni tra le variabili quantitative (compresa la risposta):

```{r echo=FALSE}
plot <- ggplot() +
  theme_void()
riquadro <- rectGrob(gp = gpar(fill = "white", col = "black"))
plot <- plot + annotation_custom(riquadro, xmin = 0, xmax = 1, ymin = 0, ymax = 1)
title1 <- plot + annotation_custom(ggplotGrob(
  ggdraw() +
    draw_label(
      "TPY",
      fontface = 'bold',
      x = 0.5,
      y = 0.5,
      hjust = 0.5,
      vjust = 0.5,
      size = 20
    )
), xmin = 0, xmax = 1, ymin = 0, ymax = 1)
title2 <- plot + annotation_custom(ggplotGrob(
  ggdraw() +
    draw_label(
      "NUMBED",
      fontface = 'bold',
      x = 0.5,
      y = 0.5,
      hjust = 0.5,
      vjust = 0.5,
      size = 20
    )
), xmin = 0, xmax = 1, ymin = 0, ymax = 1)
title3 <- plot + annotation_custom(ggplotGrob(
  ggdraw() +
    draw_label(
      "SQRFOOT",
      fontface = 'bold',
      x = 0.5,
      y = 0.5,
      hjust = 0.5,
      vjust = 0.5,
      size = 20
    )
), xmin = 0, xmax = 1, ymin = 0, ymax = 1)
cor1 <- plot + annotation_custom(ggplotGrob(
  ggdraw() +
    draw_label(
    round(cor(Data$TPY, Data$NUMBED), digits = 3),
      fontface = 'bold',
      x = 0.5,
      y = 0.5,
      hjust = 0.5,
      vjust = 0.5,
      size = 20,
    color = "red"
    )
), xmin = 0, xmax = 1, ymin = 0, ymax = 1)
cor2 <- plot + annotation_custom(ggplotGrob(
  ggdraw() +
    draw_label(
    round(cor(DataNa$TPY, DataNa$SQRFOOT), digits = 3),
      fontface = 'bold',
      x = 0.5,
      y = 0.5,
      hjust = 0.5,
      vjust = 0.5,
      size = 20,
    color = "red"
    )
), xmin = 0, xmax = 1, ymin = 0, ymax = 1)
cor3 <- plot + annotation_custom(ggplotGrob(
  ggdraw() +
    draw_label(
    round(cor(DataNa$NUMBED, DataNa$SQRFOOT), digits = 3),
      fontface = 'bold',
      x = 0.5,
      y = 0.5,
      hjust = 0.5,
      vjust = 0.5,
      size = 20,
    color = "red"
    )
), xmin = 0, xmax = 1, ymin = 0, ymax = 1)
sc1 <- plot + annotation_custom(ggplotGrob(
  ggplot(data = Data, aes(x = NUMBED, y = TPY)) +
   geom_point(shape=1) +
   theme_void() +
   xlab("") +
   ylab("") +
   geom_smooth(se = F, method = 'loess', formula = 'y ~ x', lwd = 0.75, col = "red") +
  theme(axis.ticks.x = element_blank(),
        axis.text.x = element_blank(),
        axis.ticks.y = element_blank(),
        axis.text.y = element_blank())), xmin = 0, xmax = 1, ymin = 0, ymax = 1)
sc2 <- plot + annotation_custom(ggplotGrob(
  ggplot(data = DataNa, aes(x = SQRFOOT, y = TPY)) +
   geom_point(shape=1) +
   theme_void() +
   xlab("") +
   ylab("") +
   geom_smooth(se = F, method = 'loess', formula = 'y ~ x', lwd = 0.75, col = "red") +
  theme(axis.ticks.x = element_blank(),
        axis.text.x = element_blank(),
        axis.ticks.y = element_blank(),
        axis.text.y = element_blank())), xmin = 0, xmax = 1, ymin = 0, ymax = 1)
sc3 <- plot + annotation_custom(ggplotGrob(
  ggplot(data = DataNa, aes(x = SQRFOOT, y = NUMBED)) +
   geom_point(shape=1) +
   theme_void() +
   xlab("") +
   ylab("") +
   geom_smooth(se = F, method = 'loess', formula = 'y ~ x', lwd = 0.75, col = "red") +
  theme(axis.ticks.x = element_blank(),
        axis.text.x = element_blank(),
        axis.ticks.y = element_blank(),
        axis.text.y = element_blank())), xmin = 0, xmax = 1, ymin = 0, ymax = 1)
plot_grid(title1, sc1, sc2,
          cor1, title2, sc3,
          cor2, cor3, title3,
          nrow = 3)
```

Si può subito vedere come ci sia una correlazione quasi totale tra il *TPY* (totale pazienti annui) e il *NUMBED* (numero di letti). Si può osservare ,inoltre, un'alta correlazione positiva tra il totale pazienti annui e i piedi quadrati della struttura *(SQRFOOT*), così come tra i piedi quadrati della struttura e il numero di letti. Ciò suggerisce che tutte queste variabili possano essere legate tra loro da una relazione lineare.


## Variabili Categoriche

Di seguito una breve analisi delle variabili categoriche, in primo luogo delle loro distribuzioni di frequenza.

```{r echo=FALSE}
p1 <- ggplot(data = Data, aes(x = CRYEAR, fill = CRYEAR)) +
  geom_bar(col = "black")+
  scale_fill_manual(values = heat.colors(2)) +
  theme_classic() +
  theme(legend.position = "none") +
  ylab("")

p2 <- ggplot(Data[Data$MSA != 0,], aes(x=MSA, fill=MSA)) +
  geom_bar(col = "black")+
  scale_fill_manual(values = heat.colors(13)) + 
  theme_classic() +
  theme(legend.position = "") +
  ylab("")

p3 <- ggplot(data = Data, aes(x = URBAN, fill = URBAN)) +
  geom_bar(col = "black")+
  scale_fill_manual(values = heat.colors(2)) +
  theme_classic() + 
  theme(legend.position = "none") +
  ylab("")

p4 <- ggplot(data = Data, aes(x = PRO, fill = PRO)) +
  geom_bar(col = "black")+
  scale_fill_manual(values = heat.colors(2)) +
  theme_classic() + 
  theme(legend.position = "none") +
  ylab("")

p5 <- ggplot(data = Data, aes(x = TAXEXEMPT, fill = TAXEXEMPT)) +
  geom_bar(col = "black")+
  scale_fill_manual(values = heat.colors(2)) +
  theme_classic() + 
  theme(legend.position = "none") +
  ylab("")

p6 <- ggplot(data = Data, aes(x = SELFFUNDINS, fill = SELFFUNDINS)) +
  geom_bar(col = "black")+
  scale_fill_manual(values = heat.colors(2)) +
  theme_classic() + 
  theme(legend.position = "none") +
  ylab("")

p7 <- ggplot(data = Data, aes(x = MCERT, fill = MCERT)) +
  geom_bar(col = "black")+
  scale_fill_manual(values = heat.colors(2)) +
  theme_classic() +
  theme(legend.position = "none") +
  ylab("")

p8 <- ggplot(data = Data, aes(x = ORGSTR, fill = ORGSTR)) +
  geom_bar(col = "black")+
  scale_fill_manual(values = heat.colors(3)) +
  theme_classic() +  
  theme(legend.position = "none") +
  ylab("")


# Grafico di tutti i grafici delle variabili categoriali
plot_grid(p1,p2,p3,p4,
          p5,p6,p7,p8,
          nrow = 2)
```

Abbiamo deciso, per quanto riguarda la variabile *MSA*, di non includere graficamente le osservazioni appartenenti alla classe "0", ovvero le case di riposo poste in zone rurali, poiché già rappresentate nella variabile *URBAN*.


Andiamo anche a studiare le distribuzioni condizionate della variabile risposta:    

```{r echo=FALSE}
c4 <- ggplot(data = Data, aes(x = CRYEAR, y = TPY, fill = CRYEAR)) +
  geom_boxplot() +
  scale_fill_manual(values = heat.colors(2)) +
  theme_classic() +
  theme(legend.position = "")

c5 <- ggplot(data = Data, aes(x = URBAN, y = TPY, fill = URBAN)) +
  geom_boxplot() +
  theme_classic() +
  theme(legend.position = "")+
  scale_fill_manual(values = heat.colors(2))

c6 <- ggplot(data = Data, aes(x = PRO, y = TPY, fill = PRO)) +
  geom_boxplot() +
  theme_classic() +
  theme(legend.position = "")+
  scale_fill_manual(values = heat.colors(2))

c7 <- ggplot(data = Data, aes(x = ORGSTR, y = TPY, fill = ORGSTR)) +
  geom_boxplot() +
  theme_classic() +
  theme(legend.position = "")+
  scale_fill_manual(values = heat.colors(3))

c8 <- ggplot(data = Data, aes(x = TAXEXEMPT, y = TPY, fill = TAXEXEMPT)) +
  geom_boxplot() +
  theme_classic() +
  theme(legend.position = "")+
  scale_fill_manual(values = heat.colors(2))

c9 <- ggplot(data = Data, aes(x = SELFFUNDINS, y = TPY, fill = SELFFUNDINS)) +
  geom_boxplot() +
  theme_classic() +
  theme(legend.position = "")+
  scale_fill_manual(values = heat.colors(2))

c10 <- ggplot(data = Data, aes(x = MCERT, y = TPY, fill = MCERT)) +
  geom_boxplot() +
  theme_classic() +
  theme(legend.position = "")+
  scale_fill_manual(values = heat.colors(2))

c11 <- ggplot(data = Data, aes(x = MSA, y = TPY, fill = MSA)) +
  geom_boxplot() +
  theme_classic() +
  theme(legend.position = "")+
  scale_fill_manual(values = heat.colors(14))

plot_grid(c4, c5, c6, c7,
          c8, c9, c10, c11,
          nrow = 2)
```

```{r include=FALSE}
resiplot <- function(fit, p) {
  #p è il primo grafico in alto a sinistra, così non c'è il rischio che gli si passi un p a caso
  
  # sopprimo il seguente output temporaneamente

  #per capire che data set utilizzare
  if(length(fitted.values(fit)) == 717){
    d = Data
    #print("resi plot utilizzando Data")
  }
  if(length(fitted.values(fit)) == 707){
    d = DataNa
    #print("resi plot utilizzando DataNa")
  }
  
  f <- ggplot(data = d, aes(x = fitted.values(fit), y = resid(fit))) +
    geom_point(shape=1) +
    theme_bw() +
    xlab("Valori fittati") +
    ylab("Residui") +
    geom_hline(yintercept = 0, col = "black", lty = 2) +
    geom_smooth(se = F, method = 'loess', formula = 'y ~ x', lwd = 0.75, col = "red")
  f1 <- ggplot(data = d, mapping = aes(resid(fit))) +
    geom_histogram(aes(y =after_stat(density)),bins = 20, col = "black", fill = "yellow", alpha = 1) + 
    geom_density(linewidth = 0.8, fill = "pink", alpha = 0.3) +
    theme_bw() +
    xlab("Residui") +
    ylab("Densità")
  f2 <- ggplot(data.frame(resid = rstandard(fit)),aes(sample = resid)) + 
    stat_qq(shape=1) +
    stat_qq_line(color = "red", linewidth = 1) +
    theme_bw() +
    xlab("Quantili teorici normale") +
    ylab("Quantili empirici")

  plot_grid(p, f,
            f1, f2,
            nrow = 2)
}
```

## Regressioni Lineari Semplici per la Stima di TPY

I due modelli di regressione semplice sono con le variabili *NUMBED* e *SQRFOOT*, in quanto si tratta delle uniche variabili quantitative del dataset.

### Costruzione del Modello TPY ~ NUMBED

```{r echo=FALSE}
fit_NUMBED <- lm(TPY ~ NUMBED, data = Data)
summary(fit_NUMBED)
```

Studiando i valori ritornati dal summary, si può subito vedere come questo modello sia ottimo: per quanto riguarda i residui, la mediana è molto vicina allo zero, il primo e il terzo quartile sono disposti abbastanza simmetricamente rispetto lo zero. Con minimo e massimo, invece, non si può fare lo stesso discorso e sono molto lontani dai quartili della distribuzione.     
Nel summary possiamo, poi, individuare l'intercetta e il coefficiente angolare: l'intercetta vale -0.8778 (valore vicino allo zero, il che ha senso perché nel caso di 0 letti ci si aspetta un numero di pazienti vicino allo zero) mentre il coefficiente angolare è uguale a 0.9272 (valore vicino all'1, infatti per ogni posto letto in più ci si aspetta un paziente in più).
Sull'intercetta si può fare un'ulteriore considerazione: lo standard error è 0.6925, quasi quanto l'intercetta; risulta infatti, dal test del t-value, che l'ipotesi nulla che il coefficiente sia nullo non può essere rifiutata.
Infine, studiando la statistica $R^2$, possiamo vedere un valore pari a 0.9678, il quale suggerisce un'aderenza del modello ai dati molto alta, quasi totale, circa del 97%.

#### Costruzione del Modello TPY ~ NUMBED senza intercetta

Ora proviamo a costruire il modello senza intercetta, così da stabilire se possa essere migliore:

```{r, echo=FALSE}
fit_NUMBED <- lm(TPY ~ NUMBED -1, data = Data)
summary(fit_NUMBED)
```

Il modello sembra essere migliorato, considerando che il coefficiente dell'$R^2$ è salito al 99.29%.
Dal momento che risulta plausibile e sensato che una casa di riposo con 0 posti letto ospiti 0 pazienti, accettiamo questo modello senza intercetta.
$$TPY = 0.920075\times NUMBED$$
Rappresentiamo graficamente la regressione e i suoi residui, così da poterli analizzare:

```{r echo=FALSE}
p <- ggplot(data = Data, aes(x = NUMBED, y = TPY)) +
   geom_point(shape=1) +
   theme_bw() +
   xlab("NUMBED") +
   ylab("TPY") +
   geom_smooth(se = F, method = 'lm', formula = 'y ~ x', lwd = 0.75, col = "red") +
   geom_point(aes(x = Data[564,'NUMBED'], y = Data[564,'TPY']), colour = "red", size = 1)
resiplot(fit_NUMBED, p)
```

Dallo scatterplot dato dalla relazione tra *TPY* come variabile risposta e *NUMBED* come variabile esplicativa, si vede subito come sia presente una relazione lineare tra le due.    
Dal grafico dei Residuals vs Fitted si può notare come, nonostante siano presenti degli outliers al di sotto della curva di regressione, i residui si dispongono in maniera per lo più simmetrica, suggerendo la linearità del modello.
Anche l'omoschedasticità sembra essere soddisfatta, a meno di una varianza leggermente minore per gli ospedali con meno persone, ma poi sembra stabilizzarsi.    
Gli altri due grafici, invece, non ci permettono di affermare lo stesso per quanto riguarda la gaussianità; infatti le code sono molto più pesanti di una normale, specialmente la coda inferiore.
Il modello, tuttavia, è comunque più che accettabile, nonostante la non-normalità dei residui.    
Eventuali trasformazioni per rendere i residui gaussiani rendono il modello maggiormente eteroschedastico, quindi le rifiutiamo in favore di un modello più omoschedastico e facile da interpretare.


### Costruzione del Modello TPY ~ SQRFOOT

Analizziamo ora il secondo modello con *SQRFOOT* come variabile esplicativa:

```{r echo=FALSE}
fit_SQRFOOT <- lm(TPY ~ SQRFOOT, data = Data)
summary(fit_SQRFOOT)
```

Studiando i residui dal summary, si può vedere questi si dispongano simmetricamente rispetto lo zero, nonostante fra il minimo ed il massimo ci sia una differenza di 240, molto maggiore rispetto al range interquartilico e alla distanza riscontrata nel modello *TPY* ~ *NUMBED*.
La mediana si dispone comunque vicino allo 0, ed il primo ed il terzo quartile valgono rispettivamente -15.391 e 15.615, oltre ad essere assolutamente simmetrici, ci dicono anche che la metà dei valori si troverà proprio in questo intervallo.    
In questo modello l'intercetta vale 33.5475 mentre il coefficiente angolare è pari a 1.1179 (molto vicino a 1, quindi circa per ogni piede quadrato c'è un paziente in più). 
Infine, l'$R^2$ risulta pari a 0.6756, quindi abbiamo un'aderenza del modello di circa il 68%, quindi di nuovo alta nonostante minore rispetto a quella del modello precedente.
La retta di regressione lineare risulta essere:
$$TPY = 33.54754 + 1.11786 \times SQRFOOT$$
Questo modello risulta comunque peggiore di quello con *NUMBED*. C'è però da considerare che, nel caso di *SQRFOOT*, il modello migliora qualora si effettui una trasformazione logaritmica della variabile risposta e della variabile esplicativa, ma verrà ripreso in seguito.

Studiamo adesso i grafici della regressione e dei residui:

```{r echo=FALSE}
p <- ggplot(data = DataNa, aes(x = SQRFOOT, y = TPY)) +
   geom_point(shape=1) +
   theme_bw() +
   xlab("SQRFOOT") +
   ylab("TPY") +
   geom_smooth(se = F, method = 'lm', formula = 'y ~ x', lwd = 0.75, col = "red") +
   geom_point(aes(x = Data[564,'SQRFOOT'], y = Data[564,'TPY']), colour = "red", size = 1)  +
   geom_point(aes(x = Data[200,'SQRFOOT'], y = Data[200,'TPY']), colour = "red", size = 1)  +
   geom_point(aes(x = Data[557,'SQRFOOT'], y = Data[557,'TPY']), colour = "red", size = 1)  
resiplot(fit_SQRFOOT, p)
```

Dallo scatterplot si può nuovamente osservare una relazione pressochè lineare nonostante peggiore rispetto al modello precedente.     
Per quanto riguarda il grafico Valori Fittati vs Residui, si può subito vedere come questo modello sia peggiore per quanto riguarda la linearità e l'omoschedasticità.    
Analizzando infine la normalità, si nota come questo modello sia migliore rispetto al precedente rispetto al modello precedente.

## Regressione Lineare Multipla con Variabili Categoriche    

Per costruire un modello di regressione multipla abbiamo provato ad aggiungere una variabile qualitativa ai modelli ottenuti precedentemente; non è stato preso in considerazione il modello che tiene conto sia di *NUMBED* sia di *SQRFOOT*, poiché le due variabili sono fortemente correlate e potrebbero generare effetti di multicollinearità.

Il primo modello proposto, con *NUMBED*, non è significativamente migliorabile con l'aggiunta di variabili categoriche, poiché la varianza dei dati è già spiegata molto bene e le nuove variabili risultano poco significative lasciando pressoché invariato l'$R^2$.

Procediamo quindi con la costruzione di un modello di regressione multipla introducendo una nuova variabile nel modello con *SQRFOOT*; in particolare, utilizzeremo il modello che considera le trasformazioni logaritmiche di *TPY* e *SQRFOOT*.

Per quanto riguarda le variabili dicotomiche, i due modelli migliori sono quelli individuati con l'introduzione delle variabili *PRO* e *TAXEXEMPT*:

```{r echo=FALSE}
fit_SQR_PRO <- lm(log(TPY) ~ log(SQRFOOT)*PRO, DataNa)
summary(fit_SQR_PRO)
```

La varianza dei residui è molta bassa, la mediana è molto vicino allo zero. Tutti i coefficienti sono molto significativi e l'$R^2$ è 0.6896, migliorato rispetto a quello ottenuto precedentemente ovvero 0.6751.

```{r echo=FALSE}
fit_SQR_TAX <- lm(log(TPY) ~ log(SQRFOOT)*TAXEXEMPT, DataNa)
summary(fit_SQR_TAX)
```

Anche qui la varianza è molto bassa, simmetrica rispetto allo zero. Tutti i coefficienti sono molto significativi, a parte *TAXEXEMPT1*. L'$R^2$ è 0.6866, migliorato rispetto a quello ottenuto precedentemente ovvero 0.6751.

Anche graficamente sembrano funzionare:

* Modello con *PRO*    
In blu sono rappresentati i punti (e la relativa retta di regressione) per *PRO* = 0, in rosso i punti e la relativa retta per *PRO* = 1.    

```{r echo=FALSE}
p <- ggplot(data = DataNa, aes(x = log(SQRFOOT), y = log(TPY), col = PRO)) +
  geom_point(show.legend = F, alpha = 0.8, shape = 1) +
  theme_bw() +
  xlab("Log(SQRFOOT)") +
  ylab("Log(TPY)") +
  geom_smooth(data = DataNa[DataNa$PRO == 0,], se = F, method = 'lm', formula = 'y ~ x', lwd = 1, col = "blue")+
  geom_smooth(data = DataNa[DataNa$PRO == 1,], se = F, method = 'lm', formula = 'y ~ x', lwd = 1, col = "red") +
  scale_color_manual(values = c("blue", "red"))
resiplot(fit_SQR_PRO, p)
```
Modello:
$$log(TPY) = 1.88 + 0.65*log(SQRFOOT) -0.45*PRO1 +0.15*log(SQRFOOT)*PRO1$$
Dai grafici dei residui si può affermare che l'omoschedasticità e la normalità sono rispettate.


* Modello con *TAXEXEMPT*
In blu sono rappresentati i punti (e la relativa retta di regressione) per *TAXEXEMPT* = 0, in rosso i punti e la retta per *TAXEXEMPT* = 1.

```{r echo=FALSE}
p <- ggplot(data = DataNa, aes(x = log(SQRFOOT), y = log(TPY), col = TAXEXEMPT)) +
  geom_point(show.legend = F, alpha = 0.8, shape = 1) +
  theme_bw() +
  xlab("Log(SQRFOOT)") +
  ylab("Log(TPY)") +
  geom_smooth(data = DataNa[DataNa$TAXEXEMPT == 0,], se = F, method = 'lm', formula = 'y ~ x', lwd = 0.75, col = "blue")+
  geom_smooth(data = DataNa[DataNa$TAXEXEMPT == 1,], se = F, method = 'lm', formula = 'y ~ x', lwd = 0.75, col = "red") +
  scale_color_manual(values = c("blue", "red"))
resiplot(fit_SQR_TAX, p)
```

Dai grafici dei residui si può affermare che l'omoschedasticità e la normalità sono rispettate.

Si osserva come entrambe le versioni siano molto valide, sembra quindi immediato pensare che il modello con la variabile ORGSTR sia ancora più valido, in quanto *ORGSTR* "unione" di *PRO* e *TAXEXEMPT* (vale 1 se "profit", 2 se "tax exempt" e 3 se "governmental unit"); tuttavia, risulta invece non significativa la differenza tra profit e governmental unit, sia per quanto riguarda l'intercetta sia per quanto riguarda l'interazione moltiplicativa.

Per maggiore significatività dei coefficienti e leggera superiorità in $R^2$ si sceglie quindi come preferibile il modello con la variabile categorica *PRO*.


## Train Set e Test Set

Per verificare la bontà dei due modelli individuati, si divide il dataset in due sottoinsiemi (casuali) composti l'uno dal 90% (Train Set) dei dati, l'altro dal restante 10% (Test Set).
La verifica avviene costruendo i due modelli che abbiamo ricavato sfruttando le osservazioni del Train Set, e poi utilizzarle per stimare le osservazioni nel Test Set.

Andremo prima ad analizzare una suddivisione specifica per ogni modello, e poi ripeteremo lo studio in maniera più aprossimato 100 volte.

### Modello Regressione Semplice TPY ~ NUMBED - 1

```{r echo=FALSE}
set.seed(69)

#Divisione data set
# 80% dei dati nel training e 20% nel test
sample <- sample(c(TRUE, FALSE), nrow(Data), replace=TRUE, prob=c(0.9,0.1))
train  <- Data[sample, ]
test   <- Data[!sample, ]

#Faccio il fit con il modello che abbiamo selezionato (che in caso si può cambiare)
fit_train <- lm(TPY ~ NUMBED - 1, train)
summary(fit_train)
```

Guardando il summary del modello applicato solo ad una parte dei dati si ritrova un risultato molto simile.    
*Modello consideranto la completezza dei dati*
$$TPY = 0.920075\times NUMBED$$
*Modello consideranto il data set train*
$$TPY = 0.918973\times NUMBED$$

```{r echo=FALSE}
#Dati predetti
pred <- predict.lm(fit_train, test)

dp <- ggplot()+
  geom_point(aes(x = test[,'TPY'], y = pred)) +
  ggtitle("Dati predetti vs dati reali") +
  geom_abline(intercept = 0, slope = 1, col = "red") +
  xlab("TPY reali") +
  ylab("TPY predetti")+
  theme_bw()

#calcolo residui
err <- abs(test[,'TPY'] - pred)
err <- err / abs(test[,'TPY'])
gr <- ggplot(data = test, aes(x = hospID, y = err)) +
  geom_abline(intercept = 0, slope = 0, col = "gray40") +
  geom_point() +
  ggtitle("Grafico degli errori relativi") +
  ylab("errore relativo")+
  scale_y_continuous(breaks = scales::pretty_breaks(n = 15))+
  theme_bw()

plot_grid(dp, gr)
```

Nel primo grafico confrontiamo i dati predetti rispetto ai dati reali del dataset test. Come si vede si avvicinano molto alla bisettrice (in rosso) del primo quadrante, ciò significa che i dati predetti assomigliano molto ai dati reali.
Nel secondo grafico sono rappresentati gli errori relativi, e come si vede la maggior parte degli errori sono molto piccoli.   

Formula errore relativo
$$ err_{rel} = \frac{|x_{vero} - x_{pred}|}{|x_{vero}|}$$

*Summary Errori*

```{r echo=FALSE}
quantile(err, probs = seq(0,1,0.1))
```

Per essere più precisi il 90% dei dati ha un errore più piccolo del 10%, sfortunatamente l'errore più grande è del 42%.    


### Modello Multivariato log(TPY) ~ log(SQRFOOT) * PRO
Facciamo adesso lo stesso studio con il secondo modello:    

```{r echo=FALSE}
set.seed(69)

#Divisione data set
# 90% dei dati nel training e 10% nel test
sample <- sample(c(TRUE, FALSE), nrow(DataNa), replace=TRUE, prob=c(0.9,0.1))
train  <- DataNa[sample, ]
test   <- DataNa[!sample, ]

#Faccio il fit con il modello che abbiamo selezionato (che in caso si può cambiare)
fit_train <- lm(log(TPY) ~ log(SQRFOOT) * PRO, train)
summary(fit_train)
```

Guardando il summary del modello applicato solo ad una parte dei dati si ritrovano di nuovo coefficienti molto simili a quelli gia ottenuti: 

* *Coefficienti modello ottenuto con la completezza dei dati*

```{r echo=FALSE}
coef(fit_SQR_PRO)
```

* *Coefficienti ottenuti con il dataset train*

```{r echo=FALSE}
coef(fit_train)
```


```{r echo=FALSE}
#Dati predetti
pred <- predict.lm(fit_train, test)

dp <- ggplot()+
  geom_point(aes(x = test[,'TPY'], y = exp(pred))) +
  ggtitle("Dati predetti vs dati reali") +
  geom_abline(intercept = 0, slope = 1, col = "red") +
  xlab("TPY reali") +
  ylab("TPY predetti")+
  theme_bw()

#calcolo residui
err <- abs(test[,'TPY'] - exp(pred))
err <- err / abs(test[,'TPY'])
gr <- ggplot(data = test, aes(x = hospID, y = err)) +
  geom_abline(intercept = 0, slope = 0, col = "gray40") +
  geom_point() +
  ggtitle("Grafico degli errori relativi") +
  ylab("errore relativo")+
  scale_y_continuous(breaks = scales::pretty_breaks(n = 10))+
  theme_bw()

plot_grid(dp, gr)
```

Nel primo grafico confrontiamo i dati predetti rispetto ai dati reali del test-set, e rispetto al modello precedente si osserva una maggiore dispersione.
Nel secondo grafico sono rappresentati gli errori relativi che, come si poteva dedurre, sono più alti.

```{r echo=FALSE}
quantile(err, probs = seq(0,1,0.1))
```

In questo caso il 40% dei dati ha un errore relativo più piccolo del 10%, il 90% dei dati ha un errore più piccolo del 52%. In questo caso l'errore arriva anche al 170%.

Se andiamo a ricordare il nostro modello considerando tutti i dati ricordiamo che la precentuale di dati spiegati è 68.96%.

```{r echo=FALSE}
quantile(err, 0.6896)
```

E l'errore relativo a quel quantile è minore del 24%. Quindi concludiamo che l'errore relativo della nostra predizione sulla percentuale di dati che il nostro modello prometteva di spiegare è al massimo del 24%.

### Esecuzione ripetuta del proceso train test
La divisione del dataset fra quello di train e di test è casuale, quindi, per controllare che non siamo stati fortunati nel caso che abbiamo studiato andremo a ripetere il processo 100 volte e ad analizzare i risultati attraverso l'$R^2$, la varianza sui residui, l'errore relativo sui residui e i coefficienti ottenuti.

#### Regressione Semplice *TPY* ~ *NUMBED* - 1

```{r echo=F, warning=FALSE}
#Facciamo un ciclo for nel quale separiamo più volte il data set in test e train
#Numero di ripetizioni
niter <- 100
#prime due righe due modi diversi per calcolare l'r^2
r <- matrix(data = 0, nrow = 2, ncol = niter)
#Martice prima riga varianza dei residui seconda riga la media dell'errore relativo
e <- matrix(data = 0, nrow = 2, ncol = niter)
for (i in 1:niter){
  #Divido il dataset
  sample <- sample(c(TRUE, FALSE), nrow(Data), replace=TRUE, prob=c(0.9,0.1))
  train  <- Data[sample, ]
  test   <- Data[!sample, ]
  
  #Faccio il modello
  fit_train <- lm(TPY ~ NUMBED - 1, train)
  
  #Predizion
  pred <- predict.lm(fit_train, test)
  
  #calcolo residui
  res <- test[,'TPY'] - pred
  #media dati
  m <- mean(test[,'TPY'])
  # Residui totali
  sst <- sum((test[,'TPY'] - m)^2)
  #calcolo devianza dei residui
  sse <- sum(res^2)
  #devianza della regression
  ssr <- sum((pred - m)^2)
  
  
  #Calcolo dell'r quadro
  #r1 <- (ssr / sst)
  r2 <- (1 - sse / sst)
  #r[1, i] <-  r1
  r[2, i] <-  r2
  r[1,i] <- coef(fit_train)
  
  #salvo i residui
  e[1, i] <- sse / length(res)
  e[2, i] <- mean(abs(res) / abs(test[,'TPY']))
}
```

```{r echo=F}
#Distribuzioni degli r quadri calcolati
#Questa formula non funziona
r21 <- ggplot(data = NULL, aes(y = r[1,])) +
  geom_boxplot() +
  ylab("R^2")+
  scale_y_continuous(breaks = scales::pretty_breaks(n = 12, bounds = FALSE))+
  theme_bw()

r22 <- ggplot(data = NULL, aes(x = r[2,])) +
  geom_boxplot() +
  scale_x_continuous(breaks = scales::pretty_breaks(n = 12, bounds = FALSE))+
  ggtitle("Distribuzione R^2")+
  xlab("")+
  geom_vline(xintercept = 0.9929, col = "gold3")+
  theme_bw()+
  theme(axis.ticks.y = element_blank(), axis.text.y = element_blank(), axis.title = element_blank())

rer <- ggplot(data = NULL, aes(x = e[2,]))+
  geom_boxplot() +
  ggtitle("Distibuzione media errori relativi")+
  theme_bw()+
  theme(axis.ticks.y = element_blank(), axis.text.y = element_blank(), axis.title = element_blank())+
  scale_x_continuous(breaks = scales::pretty_breaks(n = 7, bounds = FALSE))

plot_grid(r22, rer)
```


Per calcolare $R^2$ abbiamo usato la seguente formula
$$R^2 = 1 - \frac{dev_{res}} {dev_{tot}}$$
Come si può notare dal grafico la maggior parte dei dati si avvicina all'$R^2$ calcolato nel modello che considera la completezza dei dati, ovvero 0.9929.

Dal grafico degli errori si può notare che in media gli errori relativi sono molto piccoli, quasi tutti più piccoli del 10%.


```{r echo=FALSE}
ggplot(data = NULL, aes(x = r[1,]))+
  geom_boxplot( col = "black", fill = "blue", alpha = 0.5)+
  theme_bw()+
  theme(axis.ticks.y = element_blank(), axis.text.y = element_blank(), axis.title = element_blank())+
  scale_x_continuous(breaks = scales::pretty_breaks(n = 15, bounds = FALSE))+
  geom_vline(xintercept = 0.920075, col = "gold3", linewidth = 1)+
  ggtitle("Disrtibuzione dei coefficienti ottenuti")

```

In giallo è segnato il coefficiente della regressione considerando l'interezza dei dati.


#### Modello Multivariato log(TPY) ~ log(SQRFOOT) * PRO

```{r echo = F}
#Facciamo un ciclo for nel quale separiamo più volte il data set in test e train
#Numero di ripetizioni
niter <- 100
#prime due righe due modi diversi per calcolare l'r^2
r <- matrix(data = 0, nrow = 2, ncol = niter)
#Martice prima riga varianza dei residui seconda riga la media dell'errore relativo
e <- matrix(data = 0, nrow = 2, ncol = niter)
#matrice coefficenti
c <- e <- matrix(data = 0, nrow = 4, ncol = niter)
for (i in 1:niter){
  #Divido il dataset
  sample <- sample(c(TRUE, FALSE), nrow(DataNa), replace=TRUE, prob=c(0.9,0.1))
  train  <- DataNa[sample, ]
  test   <- DataNa[!sample, ]
  
  #Faccio il modello
  fit_train <- lm(log(TPY) ~ log(SQRFOOT)*PRO, train)
  
  #Predizion
  pred <- predict.lm(fit_train, test)
  
  #calcolo residui
  res <- test[,'TPY'] - exp(pred)
  #media dati
  m <- mean(test[,'TPY'])
  # Devianza totale
  sst <- sum((test[,'TPY'] - m)^2)
  #calcolo devianza dei residui
  sse <- sum(res^2)
  #devianza della regression
  ssr <- sum((exp(pred) - m)^2)
  
  #Calcolo dell'r quadro
  #r1 <- (ssr / sst)
  r2 <- (1 - sse / sst)
  #r[1, i] <-  r1
  r[2, i] <-  r2
  c[,i] <- coef(fit_train)
  
  #salvo i residui
  e[1, i] <- sse / length(res)
  e[2, i] <- mean(abs(res) / abs(test[,'TPY']))
}


```

```{r echo = FALSE, warning=FALSE}
r22 <- ggplot(data = NULL, aes(x = r[2,])) +
  geom_boxplot() +
  scale_x_continuous(breaks = scales::pretty_breaks(n = 12, bounds = FALSE))+
  ggtitle("Distribuzione R^2")+
  xlab("")+
  geom_vline(xintercept = 0.6896, col = "gold3")+
  theme_bw()+
  theme(axis.ticks.y = element_blank(), axis.text.y = element_blank(), axis.title = element_blank())

rer <- ggplot(data = NULL, aes(x = e[2,]))+
  geom_boxplot() +
  ggtitle("Distibuzione media errori relativi")+
  theme_bw()+
  theme(axis.ticks.y = element_blank(), axis.text.y = element_blank(), axis.title = element_blank())+
  scale_x_continuous(breaks = scales::pretty_breaks(n = 10, bounds = FALSE))

plot_grid(r22, rer)
```

Ricordiamo che l'$R^2$ ottenuto in precedenza è 0.6896, e notiamo che la nostra distribuzione si avvicina molto a quel valore.

Come abbiamo notato prima l'errore relativo in questo modello è più alto rispetto al primo ma osserviamo che le medie superano difficilmente lo 0.28.

```{r echo=FALSE}
p1 <- ggplot(data = NULL, aes(x = c[1,]))+
  geom_boxplot( col = "black", fill = "blue", alpha = 0.5)+
  theme_bw()+
  theme(axis.ticks.y = element_blank(), axis.text.y = element_blank(), axis.title = element_blank())+
  scale_x_continuous(breaks = scales::pretty_breaks(n = 15, bounds = FALSE))+
  geom_vline(xintercept = coef(fit_SQR_PRO)[1], col = "gold3", linewidth = 1)+
  ggtitle(names(coef(fit_SQR_PRO)[1]))

p2 <- ggplot(data = NULL, aes(x = c[2,]))+
  geom_boxplot( col = "black", fill = "blue", alpha = 0.5)+
  theme_bw()+
  theme(axis.ticks.y = element_blank(), axis.text.y = element_blank(), axis.title = element_blank())+
  scale_x_continuous(breaks = scales::pretty_breaks(n = 15, bounds = FALSE))+
  geom_vline(xintercept = coef(fit_SQR_PRO)[2], col = "gold3", linewidth = 1)+
  ggtitle(names(coef(fit_SQR_PRO)[2]))

p3 <- ggplot(data = NULL, aes(x = c[3,]))+
  geom_boxplot( col = "black", fill = "blue", alpha = 0.5)+
  theme_bw()+
  theme(axis.ticks.y = element_blank(), axis.text.y = element_blank(), axis.title = element_blank())+
  scale_x_continuous(breaks = scales::pretty_breaks(n = 15, bounds = FALSE))+
  geom_vline(xintercept = coef(fit_SQR_PRO)[3], col = "gold3", linewidth = 1)+
  ggtitle(names(coef(fit_SQR_PRO)[3]))

p4 <- ggplot(data = NULL, aes(x = c[4,]))+
  geom_boxplot( col = "black", fill = "blue", alpha = 0.5)+
  theme_bw()+
  theme(axis.ticks.y = element_blank(), axis.text.y = element_blank(), axis.title = element_blank())+
  scale_x_continuous(breaks = scales::pretty_breaks(n = 15, bounds = FALSE))+
  geom_vline(xintercept = coef(fit_SQR_PRO)[4], col = "gold3", linewidth = 1)+
  ggtitle("PRO1 * LOG(SQRFOOT)")

plot_grid(p1, p2,
          p3, p4)
```

Dal precedente grafico possiamo capire che i coefficienti ottenuti nel nostro test si avvicinano molto ai coefficienti reali (in giallo).

Dopo entrambe le analisi possiamo concludere che entrambi i modelli performano bene nel processo di train-test, i due casi specifici che siamo siamo andati a studiare all'inizio non erano dei casi estremi.

## Clustering

In ultimo proviamo a creare dei raggruppamenti di osservazioni nel dataset che siano omogenei secondo un determinato criterio.
Il criterio selezionato è quello che ci sembra essere il più intuitivo: la dimensione della casa di riposo.
Infatti è possibile sfruttare le variabili *NUMBED* e *SQRFOOT* per definire una dimensione della casa di riposo, sia a livello di capacità sia a livello di superficie effettiva.
La variabile *TPY* sarà inclusa in ogni raggruppamento.
Il primo confronto che viene effettuato è tra la divisione ottenuta utilizzando solo 1 delle due variabili o entrambe.
Utilizzando il metodo delle k-medie e la distanza euclidea per il calcolo della matrice delle distanza, adottando 2 centri, otteniamo i seguenti risultati:

```{r echo = F}

# Costruisco le distanze (euclidee per variabili quantitative)
dist.numbed <- daisy(scale(Data[,c("NUMBED", "TPY")]))
#as.matrix(dist.numbed)[1:5,1:5] #matrice delle distanze
dist.sqrfoot <- daisy(scale(Data[,c("SQRFOOT", "TPY")]))
#as.matrix(dist.sqrfoot)[1:5,1:5] #matrice delle distanze
# NB la funzione scale serve a standardizzare il dataset

# Provo anche con la distanza rispetto a tutte e 3 le variabili
dist.quant <- daisy(scale(DataNa[,c("NUMBED", "SQRFOOT", "TPY")]))
#as.matrix(dist.quant)[1:5,1:5] #matrice delle distanze

# Provo il clustering con il metodo delle k medie
km.numbed <- kmeans(scale(DataNa[,c("NUMBED", "TPY")]), centers = 2)
km.sqrfoot <- kmeans(scale(na.omit(Data[,c("SQRFOOT", "TPY")])), centers = 2)
km.quant <- kmeans(scale(na.omit(Data[,c("NUMBED", "SQRFOOT", "TPY")])), centers = 2)
# Numerosità nei due cluster
table(km.numbed$cluster)
table(km.sqrfoot$cluster)
table(km.quant$cluster)
# Numerosità molto asimmetriche
```

Le tre tabelle indicano le numerosità dei due cluster rispettivamente per il raggruppamento con *NUMBED*, *SQRFOOT* e entrambe.

Visualizziamo graficamente le differenze riscontrate tra il clustering bivariato e quello con tutte e tre le variabili quantitative:

```{r, echo = F}

# Dataframe di supporto per la rappresentazione grafica
data.centers.numbed <- data.frame(NUMBED = km.numbed$centers[,1]*sd(DataNa$NUMBED)+mean(DataNa$NUMBED),
                                  TPY = km.numbed$centers[,2]*sd(DataNa$TPY)+mean(DataNa$TPY))
data.centers.sqrfoot <- data.frame(SQRFOOT = km.sqrfoot$centers[,1]*sd(na.omit(Data$SQRFOOT))+mean(na.omit(Data$SQRFOOT)),
                                  TPY = km.sqrfoot$centers[,2]*sd(Data$TPY)+mean(Data$TPY))
data.centers.numbed.qt <- data.frame(NUMBED = km.quant$centers[,1]*sd(Data$NUMBED)+mean(Data$NUMBED),
                                  TPY = km.quant$centers[,2]*sd(Data$TPY)+mean(Data$TPY))
data.centers.sqrfoot.qt <- data.frame(SQRFOOT = km.quant$centers[,1]*sd(na.omit(Data$SQRFOOT))+mean(na.omit(Data$SQRFOOT)),
                                   TPY = km.quant$centers[,2]*sd(Data$TPY)+mean(Data$TPY))
# Visualizzo graficamente i clustering
n1 <- ggplot(DataNa, aes(x = NUMBED, y = TPY, col = factor(km.numbed$cluster))) +
  geom_point(shape = 1) +
  scale_color_manual(values = c("red", "orange")) + 
  theme_bw() +
  theme(legend.position = "") +
  geom_point(data = data.centers.numbed, aes(x = NUMBED, y = TPY), col = "black") +
  labs(title = "TPY e NUMBED")
n2 <- ggplot(DataNa, aes(x = NUMBED, y = TPY, col = factor(km.quant$cluster))) +
  geom_point(shape = 1) +
  scale_color_manual(values = c("red", "orange")) + 
  theme_bw() +
  theme(legend.position = "") +
  geom_point(data = data.centers.numbed.qt, aes(x = NUMBED, y = TPY), col = "black") +
  labs(title = "TPY, NUMBED e SQRFOOT")
n3 <- ggplot(DataNa, aes(x = NUMBED, y = TPY)) +
  geom_point(data = DataNa[factor(km.quant$cluster) == factor(km.numbed$cluster),], 
             mapping = aes(x = NUMBED, y = TPY),
             shape = 1, col = "yellow") +
  geom_point(data = DataNa[factor(km.quant$cluster) != factor(km.numbed$cluster),],
             mapping = aes(x = NUMBED, y = TPY),
             shape = 1, col = "red") +
  theme_bw() +
  geom_point(data = data.centers.numbed, aes(x = NUMBED, y = TPY), col = "black") +
  geom_point(data = data.centers.numbed.qt, aes(x = NUMBED, y = TPY), col = "grey")  +
  labs(title = "Differenze tra i due clustering")
n12 <- plot_grid(n1, n2,
                 nrow = 1)
plot_grid(n12,
          n3,
          ncol = 1)
s1 <- ggplot(na.omit(Data), aes(x = SQRFOOT, y = TPY, col = factor(km.sqrfoot$cluster))) +
  geom_point(shape = 1) + 
  scale_color_manual(values = c("orange", "red")) +
  theme_bw() +
  theme(legend.position = "") +
  geom_point(data = data.centers.sqrfoot, aes(x = SQRFOOT, y = TPY), col = "black") +
  labs(title = "TPY e SQRFOOT")
s2 <- ggplot(na.omit(Data), aes(x = SQRFOOT, y = TPY, col = factor(km.quant$cluster))) +
  geom_point(shape = 1) + 
  scale_color_manual(values = c("red", "orange")) +
  theme_bw() +
  theme(legend.position = "") +
  geom_point(data = data.centers.sqrfoot, aes(x = SQRFOOT, y = TPY), col = "black") +
  labs(title = "TPY, NUMBED e SQRFOOT")
s3 <- ggplot(DataNa, aes(x = SQRFOOT, y = TPY)) +
  geom_point(data = DataNa[factor(km.quant$cluster) != factor(km.sqrfoot$cluster),], 
             mapping = aes(x = SQRFOOT, y = TPY),
             shape = 1, col = "yellow") +
  geom_point(data = DataNa[factor(km.quant$cluster) == factor(km.sqrfoot$cluster),],
             mapping = aes(x = SQRFOOT, y = TPY),
             shape = 1, col = "red") +
  theme_bw() +
  geom_point(data = data.centers.sqrfoot, aes(x = SQRFOOT, y = TPY), col = "black") +
  geom_point(data = data.centers.sqrfoot.qt, aes(x = SQRFOOT, y = TPY), col = "grey") +
  labs(title = "Differenze tra i due clustering")
s12 <- plot_grid(s1, s2,
                 nrow = 1)
plot_grid(s12,
          s3,
          ncol = 1)
```

Si osservano in rosso i punti che appartengono a due cluster diversi a seconda del tipo di clustering adottato, in grigio i centroidi dei cluster ottenuti utilizzando tutte e tre le variabili.

Visualizziamo le varianze within e between dei tre metodi, ed osserviamo che il metodo che utilizza esclusivamente SQRFOOT ha varianza within minore in favore di una varianza between maggiore.
Visti i risultati, scegliamo di proseguire con il clustering utilizzando soltanto le variabili *TPY* e *SQRFOOT*.
Vediamo qual è il numero migliore di cluster in cui dividere il dataset:

```{r, echo = F}
crit.s<-0
for (i in 2:10) {
  set.seed(69)
  group.sqrfoot<-kmeans(scale(na.omit(Data[,c("SQRFOOT", "TPY")])), i, nstart=10)
  crit.s[i-1]<-group.sqrfoot$tot.withinss
}
clsdb <- data.frame(index = 2:10, crit.s)
ggplot(clsdb, aes(index, crit.s)) +
  geom_point() +
  geom_line() +
  theme_bw()
```

A giudicare dal grafico rappresentante la varianza between rispetto ai centroidi ottenuti aumentando il numero di centroidi, sembra essere una buona idea provare ad effettuare del clustering utilizzando 3 centroidi.

```{r echo = F}
km.sqrfoot <- kmeans(scale(na.omit(Data[,c("SQRFOOT", "TPY")])), centers = 3)
table(km.sqrfoot$cluster)

```

Visualizziamo a schermo le numerosità dei 3 centroidi: sono fortemente asimmetriche, tuttavia è un risultato accettabile considerando l'interpretazione come:   

* 1: Case di riposo di grandi dimensioni
* 2: Case di riposo di medie dimensioni
* 3: Case di riposo di piccole dimensioni

```{r, echo = F}
data.centers.sqrfoot <- data.frame(SQRFOOT = km.sqrfoot$centers[,1]*sd(na.omit(Data$SQRFOOT))+mean(na.omit(Data$SQRFOOT)),
                                   TPY = km.sqrfoot$centers[,2]*sd(Data$TPY)+mean(Data$TPY))
ggplot(na.omit(Data), aes(x = SQRFOOT, y = TPY, col = factor(km.sqrfoot$cluster))) +
  geom_point() + 
  theme_bw() +
  theme(legend.position = "") +
  geom_point(data = data.centers.sqrfoot, aes(x = SQRFOOT, y = TPY), col = "black") +
  scale_color_manual(values = heat.colors(3))
```


#### Clustering alternativo

Un altro possibile approccio per il clustering può essere non includere la variabile TPY e vedere come si comporta nei vari cluster.
Costruiamo nuovamente i cluster con il metodo delle k-medie con 3 centroidi.
Stavolta utilizziamo le variabili NUMBED e SQRFOOT, costruendo le matrici delle distanze con la distanza euclidea.
Seguono le numerosità nei 3 cluster e la rappresentazione grafica:
```{r echo = F}
dist.ns <- daisy(scale(DataNa[,c("NUMBED", "SQRFOOT")]))
km.ns <- kmeans(scale(DataNa[,c("NUMBED", "SQRFOOT")]), centers = 3)
table(km.ns$cluster)
data.centers.ns <- data.frame(SQRFOOT = km.ns$centers[,1]*sd(na.omit(Data$SQRFOOT))+mean(na.omit(Data$SQRFOOT)),
                                   NUMBED = km.ns$centers[,2]*sd(Data$NUMBED)+mean(Data$NUMBED))
ggplot(na.omit(Data), aes(x = SQRFOOT, y = NUMBED, col = factor(km.ns$cluster))) +
  geom_point() + 
  theme_bw() +
  theme(legend.position = "") +
  geom_point(data = data.centers.ns, aes(x = SQRFOOT, y = NUMBED), col = "black") +
  scale_color_manual(values = heat.colors(3))
```

Vediamo come si distribuisce TPY rispetto a questi cluster

```{r, echo = F}
g1 <- ggplot(DataNa, aes(x = factor(km.ns$cluster), y = TPY, fill = factor(km.ns$cluster))) +
  geom_boxplot() +
  theme_bw() +
  theme(legend.position = "") +
  scale_fill_manual(values = heat.colors(3)) +
  labs(x = "")
g2 <- ggplot(DataNa, aes(x = TPY, fill = factor(km.ns$cluster))) +
  geom_histogram(bins = 20, col = "black") +
  theme_bw() +
  theme(legend.position = "") +
  scale_fill_manual(values = heat.colors(3)) +
  labs(x = "", y = "frequenza assoluta")
plot_grid(g1, g2,
          nrow = 1)
```


## Conclusioni

Per concludere, dalla nostra analisi della regressione lineare con la variabile *TPY* come variabile di risposta abbiamo potuto osservare come ovviamente il modello *TPY* ~ *NUMBED* senza tenere conto dell'intercetta sia il modello più affidabile, con una aderenza dei dati al modello del circa 99%. Nonostante ciò, abbiamo comunque deciso di analizzare l'altro modello, composto da *log(TPY)* ~ *log(SQRFOOT)* * *PRO*, il quale, nononostante peggiore rispetto al primo, era comunque soddisfacente.    
Quindi, si capisce come, grazie anche all'analisi di raggruppamento, il numero di pazienti annui, o *TPY*, cresca linearmente rispetto alla grandezza di un ospedale, grandezza ovviamente legata anche al numero di posti letto presenti.